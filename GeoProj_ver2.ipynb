{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir new_folder\n",
        "%cd /content/new_folder/\n",
        "\n",
        "# download all zipped compressed folders\n",
        "\n",
        "# Master Plan 2019 Subzone Boundary (No Sea)\n",
        "!wget -O data4.zip https://data.gov.sg/dataset/c754450d-ecbd-4b7d-8dc1-c07ee842c6d1/download\n",
        "# Master Plan 2019 Planning Area Boundary\n",
        "!wget -O data5.zip https://data.gov.sg/dataset/40267ab6-7c08-45c4-b777-a3b10e68f1c8/download\n",
        "# Electoral Boundary 2020\n",
        "!wget -O data6.zip https://data.gov.sg/dataset/6241ae7f-6dfe-4351-8570-611357d1a90e/download\n",
        "# unzip all\n",
        "!unzip data4.zip && unzip data5.zip && unzip data6.zip && unzip electoral-boundary-dataset.kmz\n",
        "\n",
        "# CDC dataset from YY\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "!gdown 1hPxde9qZwt297SsBnkDyb9k4YoYJWDz7\n",
        "!gdown 127OyOlsGJV5sOX0ej3dINhE8c7gn3_sE\n",
        "!gdown 1tv32HHtH3A1ZKcj76xvXZavShCIZmjYG\n",
        "!gdown 1ZLTC3j8gtfiYYih5ffDpP0QbECtOTFrY\n",
        "!gdown 1Q51nrjV-WkdiJDJSOapSkNg4cE6xTBSI\n",
        "\n",
        "# rename files to make it clearer\n",
        "\n",
        "# data4\n",
        "!mv master-plan-2019-subzone-boundary-no-sea-kml.kml URA2019-Subzone.kml   \n",
        "!mv master-plan-2019-subzone-boundary-no-sea-geojson.geojson URA2019-Subzone.geojson   \n",
        "\n",
        "# data5\n",
        "# planning-boundary-area.kml\n",
        "\n",
        "# data6\n",
        "!mv 62C4422C0D5147ED8C28FA94627357DB.xsl electoral2020.xsl\n",
        "!mv doc.kml electoral2020.kml\n",
        "\n",
        "# data7\n",
        "# PA_CDC_Boundary_2020.kml\n",
        "\n",
        "# download 200k addresses/postal codes\n",
        "!gdown 1AiVKnBjWelL4O7nUCFBRg99Ns3i9kM_n\n",
        "\n",
        "# download YN RHS files\n",
        "!gdown 1-8pEFsIQqHQKHWhUyGDIctv6t2NwML9J\n",
        "!gdown 1-9dwefd81qvej2C0COgA8sy0-oiNgMzd\n",
        "!gdown 1-C7KFfEnAj_rLnZM2N_ZgL8-8l-ewJFp\n",
        "\n",
        "# download YN RHS polyclinics\n",
        "!gdown 1oC79akiCQSuqmkn2Pp6wlQl0Jx2b7QEG\n",
        "# download 2020 MOH RHS Excel file\n",
        "!gdown 1-3vpMQgSYBlahvCb6pfeKshGueQmOmsu\n",
        "\n",
        "# converts kml files to json files\n",
        "!pip install kml2geojson\n",
        "\n",
        "!k2g -sf PA_CDC_Boundary.json PA_CDC_Boundary_2020.kml ./\n",
        "# !k2g -sf URA2019-Plan.json URA2019-Plan.kml ./\n",
        "!k2g -sf URA2019-Subzone.json URA2019-Subzone.kml ./\n",
        "!k2g -sf electoral2020.json electoral2020.kml ./\n",
        "\n",
        "# remove unnecessary files, careful this might remove all other existing files\n",
        "# please remember to include \"!mkdir new_folder\" and \"cd new_folder\"\n",
        "import os\n",
        "contents = os.listdir()\n",
        "for i in contents:\n",
        "    if (\".kml\" not in i) and (\".csv\" not in i ):\n",
        "        try:\n",
        "            os.remove(i)\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "QQkLgAx0WuWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install --quiet geopandas\n",
        "import geopandas as gpd \n",
        "import fiona\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from ipywidgets import IntProgress\n",
        "import uuid\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# mount google drive to store generated csv files\n",
        "# to-do: modify for DataBricks\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "print(os.listdir())\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "Here are the python packages and related version that we are using for Databricks\n",
        "\n",
        "geopandas 0.12.2, \n",
        "fiona 1.9.0, \n",
        "json 2.0.9, \n",
        "pandas 1.2.4 --> 1.3.5, \n",
        "numpy 1.21.6,\n",
        "prettytable 3.6.0,\n",
        "matplotlib 3.2.2,\n",
        "seaborn 0.11.2,\n",
        "ipywidgets 7.7.1,\n",
        "\n",
        "'''\n",
        "\n",
        "# import prettytable\n",
        "# import matplotlib\n",
        "# import ipywidgets\n",
        "# display(\n",
        "# \"geopandas\", gpd.__version__,\n",
        "# \"fiona\", fiona.__version__,\n",
        "# \"json\", json.__version__,\n",
        "# \"pandas\", pd.__version__,\n",
        "# \"numpy\", np.__version__,\n",
        "# \"prettytable\", prettytable.__version__,\n",
        "# \"matplotlib\", matplotlib.__version__,\n",
        "# \"seaborn\", sns.__version__,\n",
        "# \"ipywidgets\", ipywidgets.__version__,\n",
        "# )\n"
      ],
      "metadata": {
        "id": "bXAmcZyLbpkd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "e8465427-e9da-45c6-9984-be02502ee2cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nHere are the python packages and related version that we are using for Databricks\\n\\ngeopandas 0.12.2, \\nfiona 1.9.0, \\njson 2.0.9, \\npandas 1.2.4 --> 1.3.5, \\nnumpy 1.21.6,\\nprettytable 3.6.0,\\nmatplotlib 3.2.2,\\nseaborn 0.11.2,\\nipywidgets 7.7.1,\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def digit_extend(POSTAL_CODE):\n",
        "    if len(str(POSTAL_CODE)) == 5:\n",
        "      POSTAL_CODE = '0' + str(POSTAL_CODE)\n",
        "    return str(POSTAL_CODE)\n",
        "\n",
        "def add_lnglat(\n",
        "    df, \n",
        "    x_name=\"X_ADDR\", \n",
        "    y_name=\"Y_ADDR\", \n",
        "    col=\"POSTAL_ADDR\"):\n",
        "    '''\n",
        "    Converts (X_ADDR, Y_ADDR) to (longitude, latitude) using OneMap API\n",
        "    Add new columns lng & lat to df\n",
        "    '''\n",
        "    list_of_lnglat = []\n",
        "    list_of_address = []\n",
        "    # progress bar\n",
        "    print(f\"Connecting OneMap APIs for LAT/LONG coordinates conversion...\")\n",
        "    f = IntProgress(min=0, max=len(df)) # instantiate the bar\n",
        "    display(f) # display the bar\n",
        "\n",
        "    for row in df.iloc:\n",
        "        # update progress bar\n",
        "        f.value += 1\n",
        "        hdr = {\"User-Agent\": \"pandas\"}\n",
        "        try: \n",
        "            postcode = row[col]\n",
        "        except:\n",
        "            try: \n",
        "                postcode = row[\"POSTAL_CODE\"]\n",
        "            except:\n",
        "                try:\n",
        "                    postcode = row[\"SLA_PostalCode\"]\n",
        "                except:\n",
        "                    pass\n",
        "        try:\n",
        "            # when postcode variables exist\n",
        "            if len(postcode) == 6 :\n",
        "                postcode = postcode\n",
        "            else:\n",
        "                postcode = digit_extend(postcode)\n",
        "            url = f'https://developers.onemap.sg/commonapi/search?searchVal={postcode}&returnGeom=Y&getAddrDetails=Y&pageNum=1'\n",
        "            resp = requests.get(url, headers=hdr)\n",
        "            extract = json.loads(resp.content)\n",
        "            lng = extract['results'][0]['LONGITUDE']\n",
        "            lat = extract['results'][0]['LATITUDE']\n",
        "            HOUSE_BLK_NO = extract['results'][0]['BLK_NO']\n",
        "            ROAD_NAME = extract['results'][0]['ROAD_NAME']\n",
        "            BUILDING_NAME = extract['results'][0]['BUILDING']\n",
        "        except:\n",
        "            # when postcode variables do not exist\n",
        "            try:\n",
        "                # if x/y coordinates exist\n",
        "                x, y = row[x_name], row[y_name]\n",
        "                if len(x) > 0 and float(x):\n",
        "                    url = f\"https://developers.onemap.sg/commonapi/convert/3414to4326?X={x}4&Y={y}\"\n",
        "                    resp = requests.get(url, headers=hdr)\n",
        "                    extract = json.loads(resp.content)\n",
        "                    lng = extract['longitude']\n",
        "                    lat = extract['latitude']\n",
        "                    HOUSE_BLK_NO = extract['results'][0]['BLK_NO']\n",
        "                    ROAD_NAME = extract['results'][0]['ROAD_NAME']\n",
        "                    BUILDING_NAME = extract['results'][0]['BUILDING']\n",
        "                else:\n",
        "                    print('Please check function input postcode column name, x/y coordinates column names')\n",
        "                    # return df\n",
        "            except:\n",
        "                # if x/y coordinates do not exist\n",
        "                print('Please check function input postcode column name, x/y coordinates column names')\n",
        "                # return df\n",
        "        list_of_lnglat.append([lng, lat])\n",
        "        list_of_address.append([ BUILDING_NAME,\tHOUSE_BLK_NO,\tROAD_NAME])\n",
        "\n",
        "    # transpose list_of_lnglat, then list[0] is lng, list[1] is lat\n",
        "    df[[\"LONG\", \"LAT\"]] = list_of_lnglat\n",
        "    df[[\"BUILDING_NAME\",\t\"HOUSE_BLK_NO\",\t\"ROAD_NAME\"]] = list_of_address\n",
        "    return df\n",
        "\n",
        "def pipeline_with_internet_connection(\n",
        "    csv_file_path, \n",
        "    col='POSTAL_CODE'):\n",
        "    '''\n",
        "        Input: original 200k df (with all columns)\n",
        "        Output: transformed 200k df (only required information)\n",
        "    '''\n",
        "    df = pd.read_csv(csv_file_path, on_bad_lines='skip', dtype=str)\n",
        "    df = df[[col for col in df.columns if \"Unnamed\" not in col]]\n",
        "\n",
        "    # digit add postal codes, padding postcodes to 6-digit string object\n",
        "    try:\n",
        "        df[\"POSTAL_ADDR\"] = df.apply(lambda row: digit_extend(row[col]), axis=1)\n",
        "    except:\n",
        "        # if the input is only-one-column csv file\n",
        "        try:\n",
        "            if df.shape[1] == 1:\n",
        "                df[\"POSTAL_ADDR\"] = df.apply(lambda row: digit_extend(row[df.columns[0]]), axis=1)\n",
        "                df = df[[\"POSTAL_ADDR\"]]\n",
        "        except:\n",
        "            if len(df.shape) == 1:\n",
        "                df[\"POSTAL_ADDR\"] = df.apply(lambda row: digit_extend(row), axis=1)\n",
        "                df = df[[\"POSTAL_ADDR\"]]\n",
        "\n",
        "\n",
        "    # add latitude & longitude columns using OneMap API, for padded POSTAL_ADDR\n",
        "    if 'LAT' not in df.columns:\n",
        "        df = df.drop_duplicates(subset=[\"POSTAL_ADDR\"]) \n",
        "        df = add_lnglat(df, col='POSTAL_ADDR')\n",
        "    return df\n",
        "\n",
        "# ------------------\n",
        "\n",
        "# to-do: set the path to kml files for Databricks\n",
        "def kml_to_df(\n",
        "    file_name, \n",
        "    kml_file_path= ''):\n",
        "    file_name = kml_file_path + file_name\n",
        "    if \".kml\" in file_name:\n",
        "        try:\n",
        "            df = gpd.read_file(file_name, driver='KML')\n",
        "        except:\n",
        "            fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
        "            df = gpd.read_file(file_name, driver='KML')\n",
        "    elif \".csv\" in file_name:\n",
        "        df = pd.read_csv(file_name, on_bad_lines='skip')\n",
        "        df['geometry'] = gpd.GeoSeries.from_wkt(df['geometry'])\n",
        "        df = gpd.GeoDataFrame(df, geometry='geometry')\n",
        "    return df\n",
        "\n",
        "def get_attributes_value(\n",
        "    html_description, \n",
        "    attribute_name='SUBZONE_N'):\n",
        "    df = pd.DataFrame(pd.read_html(html_description)[0])\n",
        "    return df[df.iloc[:,0]==attribute_name].values[0][1]\n",
        "#------------------------------------------------------------#   \n",
        "def get_pip (\n",
        "    gdf, \n",
        "    regions, \n",
        "    new_colname=''):\n",
        "    '''\n",
        "    Point in Polygon\n",
        "    Input:\n",
        "        df - dataframe with geometry POINT\n",
        "        regions - dataframe with geometry POLYGON/MULTIPOLYGON\n",
        "    Output:\n",
        "        df - original dataframe + each row with info on regions\n",
        "    '''\n",
        "    r_list = list(regions.Name)\n",
        "    # create empty dataframe\n",
        "    df = pd.DataFrame().reindex_like(gdf).dropna()\n",
        "    # diaplay progress bar\n",
        "    max_count = len(r_list)\n",
        "    print(f\"Mapping locations against kml file for {max_count} {new_colname} boundary info...\")\n",
        "    try:\n",
        "        f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
        "        display(f) # display the bar\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    for r in r_list:\n",
        "        # update progress bar \n",
        "        try:\n",
        "            f.value += 1\n",
        "        except:\n",
        "            pass\n",
        "        #get geometry for specific region\n",
        "        pol = (regions.loc[regions.Name==r])\n",
        "        pol.reset_index(drop = True, inplace = True)\n",
        "        #identify those records from gdf that are intersecting with the region polygon\n",
        "        pip_mask = gdf.within(pol.loc[0, 'geometry'])\n",
        "        # pip_mask = gdf.within(Polygon(pol.loc[:, 'geometry']))\n",
        "        gdf.loc[pip_mask, new_colname] = r\n",
        "    # Attribute N.A. values will be considered during boundary mapping function\n",
        "    return gdf\n",
        "\n",
        "def get_attributes_mapping(\n",
        "    gdf, \n",
        "    kml_file_name_list=[], \n",
        "    masterplan={}, \n",
        "    kml_df_list={}):\n",
        "    # attributes mapping\n",
        "    for kml_file_name in kml_file_name_list:\n",
        "        plan = masterplan[kml_file_name] \n",
        "\n",
        "        kml_df = kml_df_list[kml_file_name]\n",
        "        if type(plan) is dict and 'kml_' in kml_df[\"Name\"].values[0]:  # kml_1, kml_2, ...\n",
        "            for col, unicode_name in plan.items():\n",
        "                print(\"Boundary Rule Extracting: \", kml_file_name)\n",
        "                # build mapping for each kml file\n",
        "                plan[col] = {}\n",
        "                for name in kml_df[\"Name\"]: \n",
        "                    description = kml_df[kml_df[\"Name\"]==name][\"Description\"].values[0] \n",
        "                    html_df = pd.DataFrame(pd.read_html(description)[0])   \n",
        "                    plan[col][name] = html_df[html_df.iloc[:,0] == unicode_name].values[0][1]  #{'kml_1': \"K_M_L\", 'kml_2': \"K_M_L\"}}  \n",
        "                \n",
        "                # convert kml_1, kml_2, ... -----> mapping -----> standard name / N.A.\n",
        "                print(\"Boundary Rule Casting: \", col, unicode_name)\n",
        "                gdf[col] = gdf[col].map(plan[col]).fillna(gdf[col])\n",
        "                    \n",
        "        elif 'kml_' in kml_df[\"Name\"].values[0]:  \n",
        "            # RHS: convert kml_1, kml_2, ... -----> mapping -----> standard name\n",
        "            for col in plan:\n",
        "                print(\"Boundary Rule Casting: \", kml_file_name, col, unicode_name, )\n",
        "                gdf.loc[gdf[col].isin(kml_df['Name'].values), col] = kml_file_name[0:-4]\n",
        "    return gdf\n",
        "\n",
        "#------------------------------------------------------------#   \n",
        "def generate_outliers(\n",
        "    df, \n",
        "    col_list=[\n",
        "          'CDC_DISTRICT_NAME',\t\n",
        "          'ELD_ELECTORAL_DIVISION_NAME',\t\n",
        "          'URA_REGION_NAME',\t\n",
        "          'URA_PLANNING_AREA_NAME',\t\n",
        "          'URA_PLANNING_SUBZONE_NAME', \n",
        "          'MOH_RHS_ZONE_NAME']):\n",
        "    # This cell is to convert empty values into \"N.A.\", and save as csv\n",
        "    queries = ''\n",
        "    for col in col_list:\n",
        "            queries += col + ' != ' + col + ' or ' + col + ' == \"N.A.\" or '\n",
        "    df = df.query(queries[:-3])\n",
        "    return df\n",
        "\n",
        "#------------------------------------------------------------#   \n",
        "def assign_na_values(\n",
        "    df, \n",
        "    col_list=[\n",
        "            'CDC_DISTRICT_NAME',\t\n",
        "            'ELD_ELECTORAL_DIVISION_NAME',\t\n",
        "            'URA_REGION_NAME',\t\n",
        "            'URA_PLANNING_AREA_NAME',\t\n",
        "            'URA_PLANNING_SUBZONE_NAME', \n",
        "            'MOH_RHS_ZONE_NAME']):\n",
        "    df.reset_index(drop = True, inplace = True)\n",
        "    for col in col_list:\n",
        "        df.loc[df.query(\n",
        "                  col + ' != ' + col\n",
        "                  ).index, col] = \"N.A.\"\n",
        "    return df\n",
        "#------------------------------------------------------------#   \n",
        "def cleaning(\n",
        "    df, \n",
        "    col_list = [               \n",
        "            'CDC_DISTRICT_NAME',\n",
        "            'ELD_ELECTORAL_DIVISION_NAME',\n",
        "            'URA_REGION_NAME',\n",
        "            'URA_PLANNING_AREA_NAME',\n",
        "            'URA_PLANNING_SUBZONE_NAME',\n",
        "            'MOH_RHS_ZONE_NAME']):\n",
        "\n",
        "    df['RECORD_CREATED_DT'] = datetime.today().strftime('%Y-%m-%d')\n",
        "    # final dropping of redundant columns\n",
        "    df = df[['LAT','LONG',\n",
        "              'BUILDING_NAME',\n",
        "              'HOUSE_BLK_NO',\n",
        "              'ROAD_NAME',\n",
        "              'POSTAL_ADDR',\n",
        "              *col_list,\n",
        "              'RECORD_CREATED_DT']]\n",
        "    return df\n",
        "#------------------------------------------------------------#   \n",
        "# Pipeline of connecting OneMap API for postal code / coordinates convertion, and boundary attributes mapping\n",
        "def pipeline_without_internet_connection(\n",
        "    csv_file_path, \n",
        "    col='POSTAL_CODE',\n",
        "    col_list = [               \n",
        "            'CDC_DISTRICT_NAME',\n",
        "            'ELD_ELECTORAL_DIVISION_NAME',\n",
        "            'URA_REGION_NAME',\n",
        "            'URA_PLANNING_AREA_NAME',\n",
        "            'URA_PLANNING_SUBZONE_NAME',\n",
        "            'MOH_RHS_ZONE_NAME'],\n",
        "      kml_file_path='',\n",
        "      kml_file_name_list=[\n",
        "            \"URA2019-Subzone.kml\", \n",
        "            \"PA_CDC_Boundary_2020.kml\", \n",
        "            \"electoral2020.kml\", \n",
        "            \"Singapore Health Services.kml\" , \n",
        "            \"National University Health System.kml\", \n",
        "            \"National Healthcare Group.kml\"],\n",
        "      masterplan = {\n",
        "            # Modify if run for certain kml boundaries\n",
        "            # Electoral District Boundary\n",
        "            \"electoral2020.kml\" : [\"ELD_ELECTORAL_DIVISION_NAME\"],\n",
        "            # URA Subzone\n",
        "            \"URA2019-Subzone.kml\" : {\"URA_PLANNING_SUBZONE_NAME\": \"SUBZONE_N\",\n",
        "                                \"URA_PLANNING_AREA_NAME\": \"PLN_AREA_N\",\n",
        "                                \"URA_REGION_NAME\": \"REGION_N\"},\n",
        "            # URA with Sea\n",
        "            'planning-boundary-area.kml': {\"URA_PLANNING_AREA_NAME\": \"PLN_AREA_N\",\n",
        "                                \"URA_REGION_NAME\": \"REGION_N\"},\n",
        "            # CDC 2020\n",
        "            \"PA_CDC_Boundary_2020.kml\" : {\"CDC_DISTRICT_NAME\": \"CDC_NAME\"},\n",
        "            # RHS Boundaries\n",
        "            \"Singapore Health Services.kml\" : [\"MOH_RHS_ZONE_NAME\"], \n",
        "            \"National University Health System.kml\" : [\"MOH_RHS_ZONE_NAME\"], \n",
        "            \"National Healthcare Group.kml\" : [\"MOH_RHS_ZONE_NAME\"] }):\n",
        "\n",
        "\n",
        "    df = pd.read_csv(csv_file_path, on_bad_lines='skip', dtype=str)\n",
        "\n",
        "    # add geopandas Point as column\n",
        "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LONG, df.LAT))\n",
        "\n",
        "    # make kml_df list\n",
        "    kml_df_list = {}\n",
        "    for kml_file_name in kml_file_name_list:\n",
        "        kml_df_list[kml_file_name] = kml_to_df(kml_file_name, kml_file_path=kml_file_path)\n",
        "\n",
        "    # mapping to get kml_n or standard name\n",
        "    for kml_file_name in kml_file_name_list:\n",
        "        kml_df = kml_df_list[kml_file_name]\n",
        "        for col in masterplan[kml_file_name]:\n",
        "            gdf = get_pip(gdf, kml_df, new_colname=col)\n",
        "\n",
        "    # attributes mapping\n",
        "    gdf = get_attributes_mapping(gdf, kml_file_name_list, masterplan, kml_df_list)\n",
        "\n",
        "    # double-check: blank records ---> 'N.A.'\n",
        "    gdf = assign_na_values(gdf)\n",
        "\n",
        "    # fill N.A. with planning-boundary-area.kml\n",
        "    kml_file_name ='planning-boundary-area.kml'\n",
        "    kml_df = kml_to_df(kml_file_name, kml_file_path=kml_file_path)\n",
        "    for item in masterplan[kml_file_name]:\n",
        "        gdf_ura_na = gdf[gdf[item]==\"N.A.\"]\n",
        "        gdf_ura_na = get_pip(gdf_ura_na, kml_df, new_colname=item)\n",
        "    \n",
        "    # mapping for ura na value improvement, and concatenate back\n",
        "    gdf_ura_na = get_attributes_mapping(gdf_ura_na, kml_file_name_list=[kml_file_name], masterplan=masterplan, kml_df_list={kml_file_name: kml_df})\n",
        "    frames = [gdf_ura_na, gdf[~gdf.index.isin(gdf_ura_na.index)]]\n",
        "    gdf  = pd.concat(frames)\n",
        "\n",
        "    gdf = cleaning(gdf, col_list=col_list)\n",
        "    return gdf\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # kml_file_path = '/content/new_folder/'\n",
        "    kml_file_path = '/content/gdrive/MyDrive/HPB GeoProj Data/'\n",
        "    # csv_file_path = '/content/new_folder/' + '200kpostal_xy.csv'\n",
        "\n",
        "    csv_file_path = '/content/gdrive/MyDrive/HPB GeoProj Data/one_col.csv'\n",
        "    new_csv_file_path = os.path.splitext(csv_file_path)[0] + '_generated.csv'\n",
        "\n",
        "    df = pipeline_with_internet_connection(csv_file_path=csv_file_path)\n",
        "    df.to_csv(new_csv_file_path, header=True, index=False)\n",
        "\n",
        "    df = pipeline_without_internet_connection(csv_file_path=new_csv_file_path, kml_file_path=kml_file_path)\n",
        "    df.to_csv(new_csv_file_path, header=True, index=False)\n",
        "\n",
        "    display(df)"
      ],
      "metadata": {
        "id": "LYHef2JDLfvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPMYUvRBZF6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}