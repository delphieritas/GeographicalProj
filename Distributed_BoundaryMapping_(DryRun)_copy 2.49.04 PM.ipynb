{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab notebooks have an idle timeout of 90 minutes and absolute timeout of 12 hours. \n",
        "Colab Pro+ supports continuous code execution for up to 24 hours if you have sufficient compute units."
      ],
      "metadata": {
        "id": "iY2o-Zmp6B8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir new_folder\n",
        "%cd /content/new_folder/\n",
        "\n",
        "# download all zipped compressed folders\n",
        "\n",
        "# Master Plan 2019 Subzone Boundary (No Sea)\n",
        "!wget -O data4.zip https://data.gov.sg/dataset/c754450d-ecbd-4b7d-8dc1-c07ee842c6d1/download\n",
        "# Master Plan 2019 Planning Area Boundary\n",
        "!wget -O data5.zip https://data.gov.sg/dataset/40267ab6-7c08-45c4-b777-a3b10e68f1c8/download\n",
        "# Electoral Boundary 2020\n",
        "!wget -O data6.zip https://data.gov.sg/dataset/6241ae7f-6dfe-4351-8570-611357d1a90e/download\n",
        "# unzip all\n",
        "!unzip data4.zip && unzip data5.zip && unzip data6.zip && unzip electoral-boundary-dataset.kmz\n",
        "\n",
        "# CDC dataset from YY\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "!gdown 1hPxde9qZwt297SsBnkDyb9k4YoYJWDz7\n",
        "!gdown 127OyOlsGJV5sOX0ej3dINhE8c7gn3_sE\n",
        "!gdown 1tv32HHtH3A1ZKcj76xvXZavShCIZmjYG\n",
        "!gdown 1ZLTC3j8gtfiYYih5ffDpP0QbECtOTFrY\n",
        "!gdown 1Q51nrjV-WkdiJDJSOapSkNg4cE6xTBSI\n",
        "\n",
        "# rename files to make it clearer\n",
        "\n",
        "# data4\n",
        "!mv master-plan-2019-subzone-boundary-no-sea-kml.kml URA2019-Subzone.kml   \n",
        "!mv master-plan-2019-subzone-boundary-no-sea-geojson.geojson URA2019-Subzone.geojson   \n",
        "\n",
        "# data5\n",
        "# planning-boundary-area.kml\n",
        "\n",
        "# data6\n",
        "!mv 62C4422C0D5147ED8C28FA94627357DB.xsl electoral2020.xsl\n",
        "!mv doc.kml electoral2020.kml\n",
        "\n",
        "# data7\n",
        "# PA_CDC_Boundary_2020.kml\n",
        "\n",
        "# download 200k addresses/postal codes\n",
        "!gdown 1AiVKnBjWelL4O7nUCFBRg99Ns3i9kM_n\n",
        "\n",
        "# download YN RHS files\n",
        "!gdown 1-8pEFsIQqHQKHWhUyGDIctv6t2NwML9J\n",
        "!gdown 1-9dwefd81qvej2C0COgA8sy0-oiNgMzd\n",
        "!gdown 1-C7KFfEnAj_rLnZM2N_ZgL8-8l-ewJFp\n",
        "\n",
        "# download YN RHS polyclinics\n",
        "!gdown 1oC79akiCQSuqmkn2Pp6wlQl0Jx2b7QEG\n",
        "# download 2020 MOH RHS Excel file\n",
        "!gdown 1-3vpMQgSYBlahvCb6pfeKshGueQmOmsu\n",
        "\n",
        "# converts kml files to json files\n",
        "!pip install kml2geojson\n",
        "\n",
        "!k2g -sf PA_CDC_Boundary.json PA_CDC_Boundary_2020.kml ./\n",
        "# !k2g -sf URA2019-Plan.json URA2019-Plan.kml ./\n",
        "!k2g -sf URA2019-Subzone.json URA2019-Subzone.kml ./\n",
        "!k2g -sf electoral2020.json electoral2020.kml ./\n",
        "\n",
        "# remove unnecessary files, careful this might remove all other existing files\n",
        "# please remember to include \"!mkdir new_folder\" and \"cd new_folder\"\n",
        "import os\n",
        "contents = os.listdir()\n",
        "for i in contents:\n",
        "    if (\".kml\" not in i) and (\".csv\" not in i ):\n",
        "        try:\n",
        "            os.remove(i)\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "QQkLgAx0WuWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install --quiet geopandas\n",
        "import geopandas as gpd \n",
        "import fiona\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from ipywidgets import IntProgress\n",
        "import uuid\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# mount google drive to store generated csv files\n",
        "# to-do: modify for DataBricks\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "print(os.listdir())\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "Here are the python packages and related version that we are using for Databricks\n",
        "\n",
        "geopandas 0.12.2, \n",
        "fiona 1.9.0, \n",
        "json 2.0.9, \n",
        "pandas 1.2.4 --> 1.3.5, \n",
        "numpy 1.21.6,\n",
        "prettytable 3.6.0,\n",
        "matplotlib 3.2.2,\n",
        "seaborn 0.11.2,\n",
        "ipywidgets 7.7.1,\n",
        "\n",
        "'''\n",
        "\n",
        "# import prettytable\n",
        "# import matplotlib\n",
        "# import ipywidgets\n",
        "# display(\n",
        "# \"geopandas\", gpd.__version__,\n",
        "# \"fiona\", fiona.__version__,\n",
        "# \"json\", json.__version__,\n",
        "# \"pandas\", pd.__version__,\n",
        "# \"numpy\", np.__version__,\n",
        "# \"prettytable\", prettytable.__version__,\n",
        "# \"matplotlib\", matplotlib.__version__,\n",
        "# \"seaborn\", sns.__version__,\n",
        "# \"ipywidgets\", ipywidgets.__version__,\n",
        "# )\n"
      ],
      "metadata": {
        "id": "bXAmcZyLbpkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def digit_extend(POSTAL_CODE):\n",
        "    if len(str(POSTAL_CODE)) == 5:\n",
        "      POSTAL_CODE = '0' + str(POSTAL_CODE)\n",
        "    return str(POSTAL_CODE)\n",
        "\n",
        "def add_lnglat(\n",
        "    df, \n",
        "    x_name=\"X_ADDR\", \n",
        "    y_name=\"Y_ADDR\", \n",
        "    col=\"POSTAL_ADDR\"):\n",
        "    '''\n",
        "    Converts (X_ADDR, Y_ADDR) to (longitude, latitude) using OneMap API\n",
        "    Add new columns lng & lat to df\n",
        "    '''\n",
        "    list_of_lnglat = []\n",
        "    list_of_address = []\n",
        "    # progress bar\n",
        "    print(f\"Connecting OneMap APIs for LAT/LONG coordinates conversion...\")\n",
        "    f = IntProgress(min=0, max=len(df)) # instantiate the bar\n",
        "    display(f) # display the bar\n",
        "\n",
        "    for row in df.iloc:\n",
        "        # update progress bar\n",
        "        f.value += 1\n",
        "        hdr = {\"User-Agent\": \"pandas\"}\n",
        "        try: \n",
        "            postcode = row[col]\n",
        "        except:\n",
        "            try: \n",
        "                postcode = digit_extend(row[\"POSTAL_CODE\"])\n",
        "            except:\n",
        "                try:\n",
        "                    postcode = digit_extend(row[\"SLA_PostalCode\"])\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        try:\n",
        "            # when postcode variables exist\n",
        "            if len(postcode) == 6 :\n",
        "                url = f'https://developers.onemap.sg/commonapi/search?searchVal={postcode}&returnGeom=Y&getAddrDetails=Y&pageNum=1'\n",
        "                resp = requests.get(url, headers=hdr)\n",
        "                extract = json.loads(resp.content)\n",
        "                lng = extract['results'][0]['LONGITUDE']\n",
        "                lat = extract['results'][0]['LATITUDE']\n",
        "                HOUSE_BLK_NO = extract['results'][0]['BLK_NO']\n",
        "                ROAD_NAME = extract['results'][0]['ROAD_NAME']\n",
        "                BUILDING_NAME = extract['results'][0]['BUILDING']\n",
        "            else:\n",
        "                print('Please check function input postcode column name, and postal code length')\n",
        "        except:\n",
        "            # when postcode variables do not exist\n",
        "            try:\n",
        "                # if x/y coordinates exist\n",
        "                x, y = row[x_name], row[y_name]\n",
        "                if len(x) > 0 and float(x):\n",
        "                    url = f\"https://developers.onemap.sg/commonapi/convert/3414to4326?X={x}4&Y={y}\"\n",
        "                    resp = requests.get(url, headers=hdr)\n",
        "                    extract = json.loads(resp.content)\n",
        "                    lng = extract['longitude']\n",
        "                    lat = extract['latitude']\n",
        "                    HOUSE_BLK_NO = extract['results'][0]['BLK_NO']\n",
        "                    ROAD_NAME = extract['results'][0]['ROAD_NAME']\n",
        "                    BUILDING_NAME = extract['results'][0]['BUILDING']\n",
        "                else:\n",
        "                    print('Please check function input postcode column name, x/y coordinates column names')\n",
        "                    return df\n",
        "            except:\n",
        "                # if x/y coordinates do not exist\n",
        "                print('Please check function input postcode column name, x/y coordinates column names')\n",
        "                return df\n",
        "        list_of_lnglat.append([lng, lat])\n",
        "        list_of_address.append([ BUILDING_NAME,\tHOUSE_BLK_NO,\tROAD_NAME])\n",
        "\n",
        "    # transpose list_of_lnglat, then list[0] is lng, list[1] is lat\n",
        "    df[[\"LONG\", \"LAT\"]] = list_of_lnglat\n",
        "    df[[\"BUILDING_NAME\",\t\"HOUSE_BLK_NO\",\t\"ROAD_NAME\"]] = list_of_address\n",
        "    return df\n",
        "\n",
        "def pipeline_with_internet_connection(\n",
        "    csv_file_path, \n",
        "    col='POSTAL_CODE'):\n",
        "    '''\n",
        "        Input: original 200k df (with all columns)\n",
        "        Output: transformed 200k df (only required information)\n",
        "    '''\n",
        "    df = pd.read_csv(csv_file_path, on_bad_lines='skip', dtype=str)\n",
        "    df = df[[col for col in df.columns if \"Unnamed\" not in col]]\n",
        "    # digit add postal codes, padding postcodes to 6-digit string object\n",
        "    try:\n",
        "        df[\"POSTAL_ADDR\"] = df.apply(lambda row: digit_extend(row[col]), axis=1)\n",
        "    except:\n",
        "        # if the input is only-one-column csv file\n",
        "        if len(df.shape) == 1:\n",
        "            try:\n",
        "                df[\"POSTAL_ADDR\"] = df.apply(lambda row: digit_extend(row), axis=1)\n",
        "                df = df[[\"POSTAL_ADDR\"]]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # add latitude & longitude columns using OneMap API, for padded POSTAL_ADDR\n",
        "    if 'LAT' not in df.columns:\n",
        "        df = df.drop_duplicates(subset=[col]) \n",
        "        df = add_lnglat(df, col='POSTAL_ADDR')\n",
        "    return df"
      ],
      "metadata": {
        "id": "LYHef2JDLfvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to-do: set the path to kml files for Databricks\n",
        "def kml_to_df(\n",
        "    file_name, \n",
        "    path= ''):\n",
        "    file_name = path + file_name\n",
        "    if \".kml\" in file_name:\n",
        "        try:\n",
        "            df = gpd.read_file(file_name, driver='KML')\n",
        "        except:\n",
        "            fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
        "            df = gpd.read_file(file_name, driver='KML')\n",
        "    elif \".csv\" in file_name:\n",
        "        df = pd.read_csv(file_name, on_bad_lines='skip')\n",
        "        df['geometry'] = gpd.GeoSeries.from_wkt(df['geometry'])\n",
        "        df = gpd.GeoDataFrame(df, geometry='geometry')\n",
        "    return df\n",
        "\n",
        "def get_attributes_value(\n",
        "    html_description, \n",
        "    attribute_name='SUBZONE_N'):\n",
        "    df = pd.DataFrame(pd.read_html(html_description)[0])\n",
        "    return df[df.iloc[:,0]==attribute_name].values[0][1]\n",
        "#------------------------------------------------------------#   \n",
        "def get_pip (\n",
        "    gdf, \n",
        "    regions, \n",
        "    new_colname=''):\n",
        "    '''\n",
        "    Point in Polygon\n",
        "    Input:\n",
        "        df - dataframe with geometry POINT\n",
        "        regions - dataframe with geometry POLYGON/MULTIPOLYGON\n",
        "    Output:\n",
        "        df - original dataframe + each row with info on regions\n",
        "    '''\n",
        "    r_list = list(regions.Name)\n",
        "    # create empty dataframe\n",
        "    df = pd.DataFrame().reindex_like(gdf).dropna()\n",
        "    # diaplay progress bar\n",
        "    max_count = len(r_list)\n",
        "    print(f\"Mapping locations against kml file for {max_count} {new_colname} boundary info...\")\n",
        "    try:\n",
        "        f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
        "        display(f) # display the bar\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    for r in r_list:\n",
        "        # update progress bar \n",
        "        try:\n",
        "            f.value += 1\n",
        "        except:\n",
        "            pass\n",
        "        #get geometry for specific region\n",
        "        pol = (regions.loc[regions.Name==r])\n",
        "        pol.reset_index(drop = True, inplace = True)\n",
        "        #identify those records from gdf that are intersecting with the region polygon\n",
        "        pip_mask = gdf.within(pol.loc[0, 'geometry'])\n",
        "        # pip_mask = gdf.within(Polygon(pol.loc[:, 'geometry']))\n",
        "        gdf.loc[pip_mask, new_colname] = r\n",
        "    # Attribute N.A. values will be considered during boundary mapping function\n",
        "    return gdf\n",
        "\n",
        "def get_attributes_mapping(\n",
        "    gdf, \n",
        "    kml_file_name_list=[], \n",
        "    masterplan={}, \n",
        "    kml_df_list={}):\n",
        "    # attributes mapping\n",
        "    for kml_file_name in kml_file_name_list:\n",
        "        plan = masterplan[kml_file_name] \n",
        "\n",
        "        kml_df = kml_df_list[kml_file_name]\n",
        "        if type(plan) is dict and 'kml_' in kml_df[\"Name\"].values[0]:  # kml_1, kml_2, ...\n",
        "            for col, unicode_name in plan.items():\n",
        "                print(\"Boundary Rule Extracting: \", kml_file_name)\n",
        "                # build mapping for each kml file\n",
        "                plan[col] = {}\n",
        "                for name in kml_df[\"Name\"]: \n",
        "                    description = kml_df[kml_df[\"Name\"]==name][\"Description\"].values[0] \n",
        "                    html_df = pd.DataFrame(pd.read_html(description)[0])   \n",
        "                    plan[col][name] = html_df[html_df.iloc[:,0] == unicode_name].values[0][1]  #{'kml_1': \"K_M_L\", 'kml_2': \"K_M_L\"}}  \n",
        "                \n",
        "                # convert kml_1, kml_2, ... -----> mapping -----> standard name / N.A.\n",
        "                print(\"Boundary Rule Casting: \", col, unicode_name)\n",
        "                gdf[col] = gdf[col].map(plan[col]).fillna(gdf[col])\n",
        "                    \n",
        "        elif 'kml_' in kml_df[\"Name\"].values[0]:  \n",
        "            # RHS: convert kml_1, kml_2, ... -----> mapping -----> standard name\n",
        "            for col in plan:\n",
        "                print(\"Boundary Rule Casting: \", kml_file_name, col, unicode_name, )\n",
        "                gdf.loc[gdf[col].isin(kml_df['Name'].values), col] = kml_file_name[0:-4]\n",
        "    return gdf\n",
        "\n",
        "#------------------------------------------------------------#   \n",
        "def generate_outliers(\n",
        "    df, \n",
        "    col_list=[\n",
        "          'CDC_DISTRICT_NAME',\t\n",
        "          'ELD_ELECTORAL_DIVISION_NAME',\t\n",
        "          'URA_REGION_NAME',\t\n",
        "          'URA_PLANNING_AREA_NAME',\t\n",
        "          'URA_PLANNING_SUBZONE_NAME', \n",
        "          'MOH_RHS_ZONE_NAME']):\n",
        "    # This cell is to convert empty values into \"N.A.\", and save as csv\n",
        "    queries = ''\n",
        "    for col in col_list:\n",
        "            queries += col + ' != ' + col + ' or ' + col + ' == \"N.A.\" or '\n",
        "    df = df.query(queries[:-3])\n",
        "    return df\n",
        "\n",
        "#------------------------------------------------------------#   \n",
        "def assign_na_values(\n",
        "    df, \n",
        "    col_list=[\n",
        "            'CDC_DISTRICT_NAME',\t\n",
        "            'ELD_ELECTORAL_DIVISION_NAME',\t\n",
        "            'URA_REGION_NAME',\t\n",
        "            'URA_PLANNING_AREA_NAME',\t\n",
        "            'URA_PLANNING_SUBZONE_NAME', \n",
        "            'MOH_RHS_ZONE_NAME']):\n",
        "    for col in col_list:\n",
        "        df.loc[df.query(\n",
        "                  col + ' != ' + col\n",
        "                  ).index, col] = \"N.A.\"\n",
        "    return df\n",
        "#------------------------------------------------------------#   \n",
        "def cleaning(\n",
        "    df, \n",
        "    col_list = [               \n",
        "            'CDC_DISTRICT_NAME',\n",
        "            'ELD_ELECTORAL_DIVISION_NAME',\n",
        "            'URA_REGION_NAME',\n",
        "            'URA_PLANNING_AREA_NAME',\n",
        "            'URA_PLANNING_SUBZONE_NAME',\n",
        "            'MOH_RHS_ZONE_NAME']):\n",
        "\n",
        "    df['RECORD_CREATED_DT'] = datetime.today().strftime('%Y-%m-%d')\n",
        "    # final dropping of redundant columns\n",
        "    df = df[['LAT','LONG',\n",
        "              'BUILDING_NAME',\n",
        "              'HOUSE_BLK_NO',\n",
        "              'ROAD_NAME',\n",
        "              'POSTAL_ADDR',\n",
        "              #  'MULTI_ADDR_IND',\n",
        "              *col_list,\n",
        "              'RECORD_CREATED_DT']]\n",
        "    return df\n",
        "#------------------------------------------------------------#   \n",
        "# Pipeline of connecting OneMap API for postal code / coordinates convertion, and boundary attributes mapping\n",
        "def pipeline_without_internet_connection(\n",
        "    csv_file_path, \n",
        "    col='POSTAL_CODE',\n",
        "    col_list = [               \n",
        "            'CDC_DISTRICT_NAME',\n",
        "            'ELD_ELECTORAL_DIVISION_NAME',\n",
        "            'URA_REGION_NAME',\n",
        "            'URA_PLANNING_AREA_NAME',\n",
        "            'URA_PLANNING_SUBZONE_NAME',\n",
        "            'MOH_RHS_ZONE_NAME'],\n",
        "      kml_file_name_list=[\n",
        "            \"URA2019-Subzone.kml\", \n",
        "            \"PA_CDC_Boundary_2020.kml\", \n",
        "            \"electoral2020.kml\", \n",
        "            \"Singapore Health Services.kml\" , \n",
        "            \"National University Health System.kml\", \n",
        "            \"National Healthcare Group.kml\"],\n",
        "      masterplan = {\n",
        "            # Modify if run for certain kml boundaries\n",
        "            # Electoral District Boundary\n",
        "            \"electoral2020.kml\" : [\"ELD_ELECTORAL_DIVISION_NAME\"],\n",
        "            # URA Subzone\n",
        "            \"URA2019-Subzone.kml\" : {\"URA_PLANNING_SUBZONE_NAME\": \"SUBZONE_N\",\n",
        "                                \"URA_PLANNING_AREA_NAME\": \"PLN_AREA_N\",\n",
        "                                \"URA_REGION_NAME\": \"REGION_N\"},\n",
        "            # URA with Sea\n",
        "            'planning-boundary-area.kml': {\"URA_PLANNING_AREA_NAME\": \"PLN_AREA_N\",\n",
        "                                \"URA_REGION_NAME\": \"REGION_N\"},\n",
        "            # CDC 2020\n",
        "            \"PA_CDC_Boundary_2020.kml\" : {\"CDC_DISTRICT_NAME\": \"CDC_NAME\"},\n",
        "            # RHS Boundaries\n",
        "            \"Singapore Health Services.kml\" : [\"MOH_RHS_ZONE_NAME\"], \n",
        "            \"National University Health System.kml\" : [\"MOH_RHS_ZONE_NAME\"], \n",
        "            \"National Healthcare Group.kml\" : [\"MOH_RHS_ZONE_NAME\"] }):\n",
        "\n",
        "\n",
        "    df = pd.read_csv(csv_file_path, on_bad_lines='skip', dtype=str)\n",
        "\n",
        "    # add geopandas Point as column\n",
        "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LONG, df.LAT))\n",
        "\n",
        "    # make kml_df list\n",
        "    kml_df_list = {}\n",
        "    for kml_file_name in kml_file_name_list:\n",
        "        kml_df_list[kml_file_name] = kml_to_df(kml_file_name)\n",
        "\n",
        "    # mapping to get kml_n or standard name\n",
        "    for kml_file_name in kml_file_name_list:\n",
        "        kml_df = kml_df_list[kml_file_name]\n",
        "        for col in masterplan[kml_file_name]:\n",
        "            gdf = get_pip(gdf, kml_df, new_colname=col)\n",
        "\n",
        "    # attributes mapping\n",
        "    gdf = get_attributes_mapping(gdf, kml_file_name_list, masterplan, kml_df_list)\n",
        "\n",
        "    # double-check: blank records ---> 'N.A.'\n",
        "    gdf = assign_na_values(gdf)\n",
        "\n",
        "    # fill N.A. with planning-boundary-area.kml\n",
        "    kml_file_name ='planning-boundary-area.kml'\n",
        "    kml_df = kml_to_df(kml_file_name)\n",
        "    for item in masterplan[kml_file_name]:\n",
        "        gdf_ura_na = gdf[gdf[item]==\"N.A.\"]\n",
        "        gdf_ura_na = get_pip(gdf_ura_na, kml_df, new_colname=item)\n",
        "    \n",
        "    # mapping for ura na value improvement, and concatenate back\n",
        "    gdf_ura_na = get_attributes_mapping(gdf_ura_na, kml_file_name_list=[kml_file_name], masterplan=masterplan, kml_df_list={kml_file_name: kml_df})\n",
        "    frames = [gdf_ura_na, gdf[~gdf.index.isin(gdf_ura_na.index)]]\n",
        "    gdf  = pd.concat(frames)\n",
        "\n",
        "    gdf = cleaning(gdf, col_list=col_list)\n",
        "    return gdf\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # csv_file_path = '/content/new_folder/' + '200kpostal_xy.csv'\n",
        "\n",
        "    csv_file_path = '/content/one_col.csv'\n",
        "    new_csv_file_path = os.path.splitext(csv_file_path)[0] + '_generated.csv'\n",
        "\n",
        "    df = pipeline_with_internet_connection(csv_file_path=csv_file_path)\n",
        "    df.to_csv(new_csv_file_path, header=True, index=False)\n",
        "\n",
        "    df = pipeline_without_internet_connection(csv_file_path=new_csv_file_path)\n",
        "    df.to_csv(new_csv_file_path, header=True, index=False)\n",
        "\n",
        "    display(df)"
      ],
      "metadata": {
        "id": "T5a6MLxuRTkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------#\n",
        "def rhs_excel(rhs_excel_file_path=\"/content/gdrive/MyDrive/RHS_2022_Excel.csv\", col=\"SLA_PostalCode\"):\n",
        "    # RHS Excel has sheet header row\n",
        "    RHS_Excel = pd.read_csv(rhs_excel_file_path, on_bad_lines='skip', dtype=str)\n",
        "    if 'Table 1' in RHS_Excel.iloc[0].index:\n",
        "        RHS_Excel = pd.read_csv(rhs_excel_file_path, on_bad_lines='skip', skiprows=1, dtype=str)\n",
        "    # # drop duplicated entries, and validate RHS Excel postcodes, 133468 rows, duplicated: 17428 rows × 5 columns\n",
        "    RHS_Excel = RHS_Excel.drop_duplicates(subset=[col]).sort_values(by=col, ascending = False)\n",
        "    return RHS_Excel\n",
        "\n",
        "def compare_rhs( \n",
        "                base_df, \n",
        "                ref_df,\n",
        "                ref_pc_col='POSTAL_ADDR',\n",
        "                ref_rhs_col='MOH_RHS_ZONE_NAME',\n",
        "                base_pc_col=\"SLA_PostalCode\", \n",
        "                base_rhs_col=\"MOH_RHS\", \n",
        "                # save_file_path=\"/content/gdrive/MyDrive/\"\n",
        "                ):\n",
        "    # RHS Boundary Robustness Analysis\n",
        "    # Compare RHS against the 2022 RHS Excel file\n",
        "    # print((ref_df[ref_pc_col].values))\n",
        "    ref_df[ref_pc_col] = ref_df[ref_pc_col].astype(str)\n",
        "    base_df[base_pc_col] = base_df[base_pc_col].astype(str)\n",
        "\n",
        "    ref_df = ref_df.set_index(ref_pc_col)\n",
        "    base_df = base_df.set_index(base_pc_col)\n",
        "\n",
        "    base_df = base_df.join(ref_df, how=\"left\")\n",
        "\n",
        "    df_failed = base_df[base_df[base_rhs_col] != base_df[ref_rhs_col]]\n",
        "    df_passed = base_df[base_df[base_rhs_col] == base_df[ref_rhs_col]]\n",
        "    # print(df_failed)\n",
        "\n",
        "    t = PrettyTable([\n",
        "        'RHS Simulation Category', \n",
        "        'Size', \n",
        "        'Ratio', \n",
        "        'Size Significant (5%)'\n",
        "        ])\n",
        "\n",
        "    t.add_row(['Passed', \n",
        "              len(df_passed),\n",
        "              len(df_passed)/len(base_df),\n",
        "              len(df_passed)/len(base_df) >=0.05\n",
        "              ])\n",
        "\n",
        "    t.add_row(['Failed', \n",
        "              len(df_failed),\n",
        "              len(df_failed)/len(base_df),\n",
        "              len(df_failed)/len(base_df) >=0.05\n",
        "              ])\n",
        "\n",
        "    queries = ref_rhs_col + ' != ' + ref_rhs_col + ' or ' + ref_rhs_col + ' == \"N.A.\"'\n",
        "    na_df = base_df.query(queries)\n",
        "\n",
        "    t.add_row(['N.A. Count', \n",
        "              len(na_df),\n",
        "              len(na_df)/len(base_df),\n",
        "              len(na_df)/len(base_df) >=0.05\n",
        "              ])\n",
        "\n",
        "    t.add_row(['Total' , \n",
        "              len(base_df),\n",
        "              1,\n",
        "              'N.A.'\n",
        "              ])\n",
        "    print(t)\n",
        "    # df_failed.to_csv(save_file_path + \"compare_rhs_diff_\" + str(uuid.uuid4()) + \".csv\", header=True, index=True)\n",
        "    # base_df.to_csv(save_file_path + \"compare_rhs_concat_\" + str(uuid.uuid4()) + \".csv\", header=True, index=True)\n",
        "    return base_df\n",
        "\n",
        "#------------------------------------------------------------#\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        "def plot_na(entries, \n",
        "            col_list = [\n",
        "                'CDC_DISTRICT_NAME',\n",
        "                'ELD_ELECTORAL_DIVISION_NAME',\n",
        "                'URA_REGION_NAME',\n",
        "                'URA_PLANNING_AREA_NAME',\n",
        "                'URA_PLANNING_SUBZONE_NAME',\n",
        "                'MOH_RHS_ZONE_NAME']):\n",
        "    # plot for all the entries containing unknown \"N.A.\" values\n",
        "    # plt.figure(figsize=(150,4))\n",
        "    plt.figure(figsize=(300,2))\n",
        "\n",
        "    count = 0\n",
        "    for i in col_list:\n",
        "        count += 1\n",
        "        ax = plt.subplot(1, entries.shape[1], count)\n",
        "\n",
        "        sns.histplot(entries[i].sort_values(ascending = False), kde=False)\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.xlabel(i, fontsize=16)\n",
        "        plt.ylabel('Frequency', fontsize=16)\n",
        "    plt.title('NA_Entries', fontsize=18)\n",
        "    plt.show()\n",
        "\n",
        "    for feature_name in col_list:\n",
        "        entries_plt = entries.query(feature_name + ' == \"N.A.\"')\n",
        "        plt.figure(figsize=(300,2))\n",
        "        count = 0\n",
        "        for i in col_list:\n",
        "            count += 1\n",
        "            ax = plt.subplot(1, entries_plt.shape[1], count)\n",
        "            sns.histplot(entries_plt[i].sort_values(ascending = False), kde=False)\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.xlabel(i, fontsize=16)\n",
        "            plt.ylabel('Frequency', fontsize=16)\n",
        "        plt.title(feature_name + '_NA_Entries', fontsize=18)\n",
        "        plt.show()\n",
        "\n",
        "def na_analysis(df, outliers='', \n",
        "                col_list=[\n",
        "                    'CDC_DISTRICT_NAME',\t\n",
        "                    'ELD_ELECTORAL_DIVISION_NAME',\t\n",
        "                    'URA_REGION_NAME',\t\n",
        "                    'URA_PLANNING_AREA_NAME',\t\n",
        "                    'URA_PLANNING_SUBZONE_NAME', \n",
        "                    'MOH_RHS_ZONE_NAME']):\n",
        "    # display a statistic table for N.A. values\n",
        "    print(\"The overall percentage of data that is missing is important. \\nGenerally, if less than 5% of values are missing then it is acceptable to ignore them. \\nSchafer ( 1999 ) asserted that a missing rate of 5% or less is inconsequential. \\nBennett ( 2001 ) maintained that statistical analysis is likely to be biased when more than 10% of data are missing.\")\n",
        "    '''\n",
        "    How much data is missing? which data is missing? what's the feature of missing data? why missing data occurs? and how to improve (procedures)?\n",
        "\n",
        "    The proportion of missing data.\n",
        "    The missing data mechanisms.\n",
        "    Patterns of missing data.\n",
        "\n",
        "    According to Rubin ( 1976 ), there are three mechanisms:\n",
        "    1. Data Missing at Random (MAR)\n",
        "    2. Missing Not at Random (MNAR)\n",
        "    3. Missing Completely at Random (MCAR)\n",
        "    '''\n",
        "    t = PrettyTable([\n",
        "        'Name', \n",
        "        'Observed Data', \n",
        "        'Ratio of Observed Data', \n",
        "        'Missing Values', \n",
        "        'Ratio of Missing Values v.s. Observed Values', \n",
        "        'Ratio of Missing Values', \n",
        "        'Missing Values Significant (5%)'\n",
        "        ])\n",
        "\n",
        "    for col in col_list:\n",
        "        demoninator = (df.shape[0] - df.query(f'{col} == \"N.A.\" or {col} != {col}').shape[0])\n",
        "        numerator = ( df.query(f'{col} == \"N.A.\" or {col} != {col}').shape[0])\n",
        "        if demoninator != 0:\n",
        "            na_ratio = numerator / demoninator\n",
        "        else:\n",
        "            na_ratio = 1\n",
        "        t.add_row([col, \n",
        "                  df.shape[0] - df.query(f'{col} == \"N.A.\" or {col} != {col}').shape[0],\n",
        "                  1- (df.query(f'{col} == \"N.A.\" or {col} != {col}').shape[0]) / (df.shape[0]),\n",
        "                  ( df.query(f'{col} == \"N.A.\" or {col} != {col}').shape[0]),\n",
        "                  na_ratio,\n",
        "                  numerator / (df.shape[0]),\n",
        "                  numerator / (df.shape[0]) >=0.05\n",
        "                  ])\n",
        "        \n",
        "    outliers = generate_outliers(df, col_list=col_list)\n",
        "    if len(outliers)>0:\n",
        "        denominator = ( (df.shape[0]) - ( outliers.shape[0] ) )\n",
        "        numerator = ( outliers.shape[0]) \n",
        "        if denominator !=0:\n",
        "            na_ratio = numerator / denominator\n",
        "        else:\n",
        "            na_ratio = 1\n",
        "        t.add_row(['Total: ' + str(df.shape[0]), \n",
        "                  (df.shape[0]) - numerator,\n",
        "                  1 - numerator / (df.shape[0]),\n",
        "                  outliers.shape[0] ,\n",
        "                  na_ratio,\n",
        "                  numerator / (df.shape[0]),\n",
        "                  numerator / (df.shape[0]) >=0.05\n",
        "                  ])\n",
        "    print(t)\n",
        "\n",
        "\n",
        "try:\n",
        "    # to-do: modify for DataBricks for outputs\n",
        "    # test_df.to_csv(\"/content/gdrive/MyDrive/GeoSpacialOutputs.csv\", header=True, index=False)\n",
        "    \n",
        "    # ---------------------------------\n",
        "    # N.A. values statistics\n",
        "    # ---------------------------------\n",
        "    na_analysis(test_df)\n",
        "    # ---------------------------------\n",
        "    # RHS Boundary Robustness Analysis\n",
        "    # verify new result (URA with Sea) against 2020 MOH RHS Excel\n",
        "    # ---------------------------------\n",
        "    rhs_excel_file_path = \"/content/new_folder/RHS_2022_Excel.csv\"\n",
        "    RHS_Excel = rhs_excel(rhs_excel_file_path)\n",
        "    compare_rhs_concat = compare_rhs(test_df, RHS_Excel, ref_pc_col=\"SLA_PostalCode\", ref_rhs_col=\"MOH_RHS\", base_pc_col=\"POSTAL_ADDR\", base_rhs_col='MOH_RHS_ZONE_NAME')\n",
        "    # check 122255 complete csv list v.s. rhs excel, vise versa\n",
        "    target_attribute = 'SLA_PostalCode'\n",
        "    postcode_list_1 = set(RHS_Excel[target_attribute].astype(str))\n",
        "    target_attribute = 'POSTAL_ADDR'\n",
        "    missing_in_rhs_excel = test_df[~test_df[target_attribute].astype(str).isin(postcode_list_1)]\n",
        "\n",
        "    target_attribute = 'POSTAL_ADDR'\n",
        "    postcode_list_2 = set(test_df[target_attribute].astype(str))\n",
        "    target_attribute = 'SLA_PostalCode'\n",
        "    missing_in_complete_csv = RHS_Excel[~RHS_Excel[target_attribute].astype(str).isin(postcode_list_2)]\n",
        "    print('Cross Checking: \\nMissing postal code out of incomplete postal code list',  '\\n',\n",
        "        len(missing_in_rhs_excel), ' / ', len(postcode_list_2), '\\n',\n",
        "        len(missing_in_complete_csv), ' / ', len(postcode_list_1)\n",
        "    ) \n",
        "\n",
        "    # Plot the N.A. value distributions\n",
        "    # for new df\n",
        "    col_list = ['CDC_DISTRICT_NAME',\n",
        "        'ELD_ELECTORAL_DIVISION_NAME',\n",
        "        'URA_REGION_NAME',\n",
        "        'URA_PLANNING_AREA_NAME',\n",
        "        'URA_PLANNING_SUBZONE_NAME',\n",
        "        'MOH_RHS_ZONE_NAME']\n",
        "    plot_na(entries = test_df, col_list=col_list)\n",
        "\n",
        "    \n",
        "except:\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "FY1dn1e_Ei-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the N.A. value distributions\n",
        "# for outliers\n",
        "col_list = ['CDC_DISTRICT_NAME',\n",
        "    'ELD_ELECTORAL_DIVISION_NAME',\n",
        "                'URA_REGION_NAME',\n",
        "                'URA_PLANNING_AREA_NAME',\n",
        "                'URA_PLANNING_SUBZONE_NAME',\n",
        "                'MOH_RHS_ZONE_NAME']\n",
        "outliers = generate_outliers(test_df, col_list=col_list)\n",
        "plot_na(entries = outliers, col_list=col_list)\n",
        "\n",
        "# Outliers distribution plots\n",
        "plt.figure(figsize=(50,5))\n",
        "sns.displot(outliers, x=\"URA_REGION_NAME\", hue=\"URA_PLANNING_AREA_NAME\") # , kde=True\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "sns.displot(outliers, x=\"URA_REGION_NAME\", hue=\"URA_PLANNING_SUBZONE_NAME\") # , kde=True\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "sns.displot(outliers, x=\"URA_PLANNING_AREA_NAME\", hue=\"URA_PLANNING_SUBZONE_NAME\") # , kde=True\n",
        "plt.xticks(rotation=90)\n",
        "plt.gcf().set_size_inches(30, 10)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Outliers density analysis \n",
        "# density in hue colors\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "sns.histplot(outliers, x=\"URA_REGION_NAME\", y='URA_PLANNING_AREA_NAME', hue=\"MOH_RHS_ZONE_NAME\", cbar=True)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "sns.histplot(outliers, x=\"URA_REGION_NAME\", y='URA_PLANNING_SUBZONE_NAME', hue=\"MOH_RHS_ZONE_NAME\", cbar=True)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "sns.histplot(outliers, x=\"URA_PLANNING_AREA_NAME\", y='URA_PLANNING_SUBZONE_NAME', hue=\"MOH_RHS_ZONE_NAME\", cbar=True)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(15,15))\n",
        "# sns.histplot(outliers, x=\"URA_REGION_NAME\", y='URA_PLANNING_AREA_NAME', hue=\"URA_PLANNING_SUBZONE_NAME\", cbar=True)\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(15,15))\n",
        "# sns.histplot(outliers, x=\"URA_REGION_NAME\", y='URA_PLANNING_SUBZONE_NAME', hue=\"URA_PLANNING_AREA_NAME\", cbar=True)\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(25,20))\n",
        "sns.histplot(outliers, x=\"URA_PLANNING_AREA_NAME\", y='URA_PLANNING_SUBZONE_NAME', hue=\"URA_REGION_NAME\", cbar=True)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "CmG8BaIcLTDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv_file_path = '/content/gdrive/MyDrive/HPB Data Results/backup_FinalRun26_subzone_n_sea.csv'\n",
        "# df = pd.read_csv(csv_file_path, on_bad_lines='skip', dtype=str)\n",
        "\n",
        "\n",
        "# eld_outliers = generate_outliers(df, \n",
        "#                        col_list=[\n",
        "#                               # 'CDC_NAME',\t\n",
        "#                               'ELD_ELECTORAL_DIVISION_NAME',\t\n",
        "#                               # 'URA_REGION_NAME',\t\n",
        "#                               # 'URA_PLANNING_AREA_NAME',\t\n",
        "#                               # 'URA_PLANNING_SUBZONE_NAME', \n",
        "#                               # 'MOH_RHS_ZONE_NAME'\n",
        "#                               ])\n",
        "# eld_outliers.to_csv('/content/gdrive/MyDrive/HPB Data Results/backup_eld_outliers.csv',  header=True, index=False)\n",
        "# display(eld_outliers)\n",
        "# print(len(eld_outliers))"
      ],
      "metadata": {
        "id": "uNOdxuNJ62mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# df[['LAT', 'LONG', 'BUILDING_NAME', 'HOUSE_BLK_NO', 'ROAD_NAME',\n",
        "#        'POSTAL_ADDR', 'MULTI_ADDR_IND', 'CDC_DISTRICT_NAME',\n",
        "#        'ELD_ELECTORAL_DIVISION_NAME', 'URA_REGION_NAME',\n",
        "#        'URA_PLANNING_AREA_NAME', 'URA_PLANNING_SUBZONE_NAME',\n",
        "#        'MOH_RHS_ZONE_NAME']].to_csv('/content/gdrive/MyDrive/HPB Data Results/FinalRun02Feb.csv', header=True, index=False)"
      ],
      "metadata": {
        "id": "Qo7JuhjXUoWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df[['BUILDING_NAME']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7et0tBL4K6i2",
        "outputId": "a7596b4f-64c9-436b-b2e9-9ac5cae47dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRY816Agsyk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}